{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training a Softmax Linear Classifier\n",
    "# http://cs231n.github.io/neural-networks-case-study/\n",
    "# we’ll walk through a complete implementation of a toy Neural Network in 2 dimensions. \n",
    "# We’ll first implement a simple linear classifier and then extend the code to a 2-layer Neural Network.\n",
    "\n",
    "# hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating some data\n",
    "# There are 2 attributes (x,y position)  The class variable has 3 values.  There are 100 samples.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "# lets visualize the data:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(300, 2)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "# SCORE FUNCTION - LINEAR   x * W + b\n",
    "# The score function (single sample) is  1x2 * 2x3 + 1x3 -->   1x3\n",
    "# for 300 samples it is                300x2 * 2x3 + 1x3 --> 300x3\n",
    "# \n",
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "print(W.shape)\n",
    "print(X.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "# Compute the class scores\n",
    "# compute class scores for a linear classifier\n",
    "# here they are computing *all* the scores.... \n",
    "#       300x2 * 2x3 -->  300x3 + 1x3(broadcasted) --> 300x3\n",
    "scores = np.dot(X, W) + b\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n",
      "1.09931289565\n"
     ]
    }
   ],
   "source": [
    "# Compute the loss\n",
    "# in this example we use the cross-entropy loss that is associated with the Softmax classifier. \n",
    "#     Li = -log(e^f(yi) / sumj(e^f(yj)))\n",
    "# the Softmax classifier interprets every element of f as holding the (unnormalized) log probabilities\n",
    "# of the three classes. We exponentiate these to get (unnormalized) probabilities, and then normalize\n",
    "# them to get probabilites. Therefore, the expression inside the log is the normalized probability of the correct class.\n",
    "#\n",
    "# the full Softmax classifier loss is then defined as the average cross-entropy loss over the training examples\n",
    "# and the regularization:\n",
    "#    L = 1/N sumi(Li)              + 1/2 lambda     sumk(suml(Wkl^2))\n",
    "#        (avg element-wise loss)         (weighted W2 regularization)\n",
    "num_examples = X.shape[0]\n",
    "# get unnormalized probabilities\n",
    "exp_scores = np.exp(scores)\n",
    "# normalize them for each example\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "print(probs.shape)\n",
    "\n",
    "# We now have an array probs of size [300 x 3], where each row now contains the class probabilities. In particular, since we’ve normalized them every row now sums to one. We can now query for the log probabilities assigned to the correct classes in each example:\n",
    "corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "\n",
    "# The array correct_logprobs is a 1D array of just the probabilities assigned to the correct classes for each example.\n",
    "# The full loss is then the average of these log probabilities and the regularization loss:\n",
    "# compute the loss: average cross-entropy loss and regularization\n",
    "data_loss = np.sum(corect_logprobs)/num_examples\n",
    "reg_loss = 0.5*reg*np.sum(W*W)\n",
    "loss = data_loss + reg_loss\n",
    "\n",
    "# In this code, the regularization strength λλ is stored inside the reg. The convenience factor of 0.5 multiplying the regularization will become clear in a second. Evaluating this in the beginning (with random parameters) might give us loss = 1.1, which is np.log(1.0/3), since with small initial random weights all probabilities assigned to all classes are about one third. We now want to make the loss as low as possible, with loss = 0 as the absolute lower bound. But the lower the loss is, the higher are the probabilities assigned to the correct classes for all examples.\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing the Analytic Gradient with Backpropagation\n",
    "# (refer to the Stanford notes here... )\n",
    "# dLi / dfk = pk - 1 (if yi = k)\n",
    "#           = pk (if yi != k)\n",
    "\n",
    "dscores = probs\n",
    "dscores[range(num_examples),y] -= 1\n",
    "dscores /= num_examples\n",
    "\n",
    "# armed with the gradient on scores (stored in dscores), we can now backpropagate into W and b:\n",
    "dW = np.dot(X.T, dscores)\n",
    "db = np.sum(dscores, axis=0, keepdims=True)\n",
    "dW += reg*W # don't forget the regularization gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a parameter update\n",
    "# Now that we’ve evaluated the gradient we know how every parameter\n",
    "# influences the loss function. We will now perform a parameter update\n",
    "# in the negative gradient direction to decrease the loss:\n",
    "\n",
    "# perform a parameter update\n",
    "W += -step_size * dW\n",
    "b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.099688\n",
      "iteration 10: loss 0.906191\n",
      "iteration 20: loss 0.833274\n",
      "iteration 30: loss 0.799645\n",
      "iteration 40: loss 0.781892\n",
      "iteration 50: loss 0.771651\n",
      "iteration 60: loss 0.765371\n",
      "iteration 70: loss 0.761347\n",
      "iteration 80: loss 0.758681\n",
      "iteration 90: loss 0.756871\n",
      "iteration 100: loss 0.755616\n",
      "iteration 110: loss 0.754733\n",
      "iteration 120: loss 0.754103\n",
      "iteration 130: loss 0.753649\n",
      "iteration 140: loss 0.753319\n",
      "iteration 150: loss 0.753077\n",
      "iteration 160: loss 0.752899\n",
      "iteration 170: loss 0.752767\n",
      "iteration 180: loss 0.752669\n",
      "iteration 190: loss 0.752595\n",
      "training accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together: Training a Softmax Classifier\n",
    "#Train a Linear Classifier\n",
    "\n",
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(200):\n",
    "  \n",
    "  # evaluate class scores, [N x K]\n",
    "  scores = np.dot(X, W) + b \n",
    "  \n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "  data_loss = np.sum(corect_logprobs)/num_examples\n",
    "  reg_loss = 0.5*reg*np.sum(W*W)\n",
    "  loss = data_loss + reg_loss\n",
    "  if i % 10 == 0:\n",
    "    print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(num_examples),y] -= 1\n",
    "  dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters (W,b)\n",
    "  dW = np.dot(X.T, dscores)\n",
    "  db = np.sum(dscores, axis=0, keepdims=True)\n",
    "  \n",
    "  dW += reg*W # regularization gradient\n",
    "  \n",
    "  # perform a parameter update\n",
    "  W += -step_size * dW\n",
    "  b += -step_size * db\n",
    "\n",
    "# evaluate training set accuracy\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.098575\n",
      "iteration 1000: loss 0.290353\n",
      "iteration 2000: loss 0.254144\n",
      "iteration 3000: loss 0.245892\n",
      "iteration 4000: loss 0.244349\n",
      "iteration 5000: loss 0.243239\n",
      "iteration 6000: loss 0.242464\n",
      "iteration 7000: loss 0.242150\n",
      "iteration 8000: loss 0.242015\n",
      "iteration 9000: loss 0.241930\n",
      "training accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Training a Neural Network\n",
    "# Clearly, a linear classifier is inadequate for this dataset and we would like\n",
    "# to use a Neural Network. One additional hidden layer will suffice for this toy data.\n",
    "# We will now need two sets of weights and biases (for the first and second layers):\n",
    "\n",
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(10000):\n",
    "  \n",
    "  # evaluate class scores, [N x K]\n",
    "  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "  scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "  data_loss = np.sum(corect_logprobs)/num_examples\n",
    "  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "  loss = data_loss + reg_loss\n",
    "  if i % 1000 == 0:\n",
    "    print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(num_examples),y] -= 1\n",
    "  dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters\n",
    "  # first backprop into parameters W2 and b2\n",
    "  dW2 = np.dot(hidden_layer.T, dscores)\n",
    "  db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "  # next backprop into hidden layer\n",
    "  dhidden = np.dot(dscores, W2.T)\n",
    "  # backprop the ReLU non-linearity\n",
    "  dhidden[hidden_layer <= 0] = 0\n",
    "  # finally into W,b\n",
    "  dW = np.dot(X.T, dhidden)\n",
    "  db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "  # add regularization gradient contribution\n",
    "  dW2 += reg * W2\n",
    "  dW += reg * W\n",
    "  \n",
    "  # perform a parameter update\n",
    "  W += -step_size * dW\n",
    "  b += -step_size * db\n",
    "  W2 += -step_size * dW2\n",
    "  b2 += -step_size * db2\n",
    "\n",
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
